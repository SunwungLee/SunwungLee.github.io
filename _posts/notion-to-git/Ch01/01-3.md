# 01-3 마켓과 머신러닝

- **Keywords**
    - **특성:** 데이터를 표현하는 하나의 성질
    - **훈련:** 머신러닝 알고리즘이 데이터에서 규칙을 찾는 과정
    - **k-최근접 이웃 알고리즘:** 가장 간단한 머신러닝 알고리즘 중 하나. 전체 데이터를 메모리에 가지고 있는 것이 전부.
    - **모델:** 머신러닝 프로그램에서 알고리즘이 구현된 객체
    - **정확도:** 정확한 답을 몇 개 맞혔는지를 백분율로 나타낸 값

한빛 마켓은 싸고 좋은 물건으로 인기가 높은 앱 마켓이다. 물건이 많아지다 보니 너무 바쁘고 직원을 계속 채용하기도 어렵다. 이를 머신러닝을 사용해 문제를 해결하기로 마음먹었다.

하지만, 물류 센터에서 생선을 고르는 직원이 도통 생선 이름을 외우지 못했다. 이는 배송 지연으로 연결되기 일쑤였다. 

머신러닝의 첫 번째 임무는 생선 이름을 자동으로 알려주는 프로그램을 만드는 것이다.

## 생선 분류 문제

- 캐글에 공개된 생선 데이터셋

    [https://www.kaggle.com/aungpyaeap/fish-market](https://www.kaggle.com/aungpyaeap/fish-market)

생선을 분류하는 일이니, 생선의 특징을 알면 쉽게 구분할 수 있을 것 같다. 

- 도미 데이터 준비하기
    1. **특성(feature)**을 matrix 형태로 준비하자.

        ```python
        # 도미 길이, 무게
        bream_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 
                        31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 
                        35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0]
        bream_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 
                        500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 
                        700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0]
        ```

    2. 두 특성을 숫자로 보는 것보다 그래프로 표현하면 데이터를 잘 이해할 수 있다.
    이를 산점도(scatter plot)로 표시해 보자.

        ```python
        import matplotlib.pyplot as plt

        plt.scatter(bream_length, bream_weight)
        plt.xlabel('length')
        plt.ylabel('weight')
        plt.show()
        ```

        ![01-3%20%E1%84%86%E1%85%A1%E1%84%8F%E1%85%A6%E1%86%BA%E1%84%80%E1%85%AA%20%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%200973347cb86f481aae39534185ca0f54/Untitled.png](01-3%20%E1%84%86%E1%85%A1%E1%84%8F%E1%85%A6%E1%86%BA%E1%84%80%E1%85%AA%20%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%200973347cb86f481aae39534185ca0f54/Untitled.png)

        위 코드의 결과

    3. 산점도 그래프가 일직선에 가까운 형태로 나타난다: 이를 선형(linear)적이라고 말한다.

- 빙어 데이터 준비하기
    1. **특성(feature)**을 matrix 형태로 준비하자.

        ```python
        # 빙어 길이, 무게
        smelt_length = [9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0]
        smelt_weight = [6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9]
        ```

    2. 두 특성을 숫자로 보는 것보다 그래프로 표현하면 데이터를 잘 이해할 수 있다.
    이를 산점도(scatter plot)로 표시해 보자.

        ```python
        import matplotlib.pyplot as plt
        plt.scatter(smelt_length, smelt_weight)
        plt.xlabel('length')
        plt.ylabel('weight')
        plt.show()
        ```

        ![01-3%20%E1%84%86%E1%85%A1%E1%84%8F%E1%85%A6%E1%86%BA%E1%84%80%E1%85%AA%20%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%200973347cb86f481aae39534185ca0f54/Untitled%201.png](01-3%20%E1%84%86%E1%85%A1%E1%84%8F%E1%85%A6%E1%86%BA%E1%84%80%E1%85%AA%20%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%200973347cb86f481aae39534185ca0f54/Untitled%201.png)

        코드 결과값

- 두 데이터 산점도 비교
    1. 이를 산점도(scatter plot)로 표시해 보자.

        ```python
        plt.scatter(bream_length, bream_weight)
        plt.scatter(smelt_length, smelt_weight)
        plt.xlabel('length')
        plt.ylabel('weight')
        plt.show()
        ```

        ![01-3%20%E1%84%86%E1%85%A1%E1%84%8F%E1%85%A6%E1%86%BA%E1%84%80%E1%85%AA%20%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%200973347cb86f481aae39534185ca0f54/Untitled%202.png](01-3%20%E1%84%86%E1%85%A1%E1%84%8F%E1%85%A6%E1%86%BA%E1%84%80%E1%85%AA%20%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%200973347cb86f481aae39534185ca0f54/Untitled%202.png)

        도비와 빙어의 데이터 비교

## 첫 번째 머신러닝 프로그램

가장 간단하고 이해하기 쉬운 k-최근접 이웃 알고리즘(K-Nearest Neighbors)을 사용해 도미와 빙어 데이터를 구분해 보자.

- 사이킷런(scikit-learn)을 사용한 모델
    1. 간단하게 두 생선의 데이터를 합쳐보자

        ```python
        # 도미 35개 + 빙어 14개
        length = bream_length + smelt_length
        weight = bream_weight + smelt_weight
        ```

    2. 각 특성의 리스트를 세로 방향으로 늘어뜨린 2차원의 리스트를 만들어야 함.
    zip() 함수와 list comprehension 구문을 사용하자.

        ```python
        fish_data = [[l, w] for l, w in zip(length, weight)]
        ```

        ![01-3%20%E1%84%86%E1%85%A1%E1%84%8F%E1%85%A6%E1%86%BA%E1%84%80%E1%85%AA%20%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%200973347cb86f481aae39534185ca0f54/Untitled%203.png](01-3%20%E1%84%86%E1%85%A1%E1%84%8F%E1%85%A6%E1%86%BA%E1%84%80%E1%85%AA%20%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%200973347cb86f481aae39534185ca0f54/Untitled%203.png)

        print(fish_data)

    3. [길이, 무게]가 하나의 리스트를 구성하고, 이런 리스트가 모여 전체 리스트를 만들었다.
    이런 리스트를 **2차원 리스트**라고 한다.
    4. 머신러닝 알고리즘이 생선의 길이와 무게를 보고 생선을 구분하는 규칙을 찾기를 원한다. → 적어도 어떤 생선이 도미인지 빙어인지 알려주어야 함.

        ```python
        # 도미 = 1, 빙어 = 0
        fish_target = [1]*35 + [0]*14
        ```

    5. 이제 간단하게 k-Nearest Neighbors 알고리즘을 구현하자.
    fit() 메서드는 주어진 데이터로 알고리즘을 훈련시킨다.

        ```python
        # k-Nearest Neighbors algorithm
        from sklearn.neighbors import KNeighborsClassifier
        kn = KNeighborsClassifier() # 객체 생성
        kn.fit(fish_data, fish_target) # 훈련(training)
        ```

    6. 이제 모델 kn이 얼마나 잘 훈련되었는지 평가.
    score() 메서드는 0~1 값을 반환한다. (e.g. 1은 모든 데이터를 정확히 맞춤)

        ```python
        kn.score(fish_data, fish_target)
        ```

    7.  결과값이 1.0이 나왔다. 모든 fish_data의 답을 정확히 맞혔다는 것을 의미한다.
    8. 이 값을 **정확도(Accuracy)**라고 부른다.
- k-Nearest Neighbors algorithm
    - 어떤 데이터에 대한 답을 구할 때, 주위의 다른 데이터를 보고 다수를 차지하는 것을 정답으로 사용한다. 마치 [근묵자흑](https://namu.wiki/w/%EA%B7%BC%EB%AC%B5%EC%9E%90%ED%9D%91)과 같다.

    ![01-3%20%E1%84%86%E1%85%A1%E1%84%8F%E1%85%A6%E1%86%BA%E1%84%80%E1%85%AA%20%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%200973347cb86f481aae39534185ca0f54/Untitled%204.png](01-3%20%E1%84%86%E1%85%A1%E1%84%8F%E1%85%A6%E1%86%BA%E1%84%80%E1%85%AA%20%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%200973347cb86f481aae39534185ca0f54/Untitled%204.png)

    - 이 삼각형 주위에 도미 데이터가 많으므로 삼각형을 도미라고 판단 할 것이다.

        ```python
        kn.predict([[30, 600]])
        ```

    - predict() method는 **새로운** 데이터의 정답을 예측한다.
    - 이렇게 생각하면 k-NN 알고리즘을 위해 준비 해야 할 일은 데이터를 모두 가지고 있는 것이 전부이다. 
    즉, 새로운 데이터에 대해 예측할 떄는 가장 가까운 직선거리에 어떤 데이터가 있는지를 살피기만 하면 된다.
    - k-NN의 단점은 **데이터가 아주 많은 경우**에 사용하기 어렵다. 
    데이터가 크기 때문에 메모리가 많이 필요하고 직선거리를 계산하는 데도 많은 시간이 필요하다.
    - 그럼 가까운 몇 개의 데이터를 참고할까?
        - 이는 정하기 나름이다.
        - KNeighborsClassifier 클래스의 기본값은 **5**이다.
        - 이는 n_neighbors 매개변수로 바꿀 수 있다.

            ```python
            kn49 = KNeighborsClassifier(n_neighbors=49) # 참고데이터를 49개로 한 모델
            ```

        - 위 코드에 대한 결과는 아래와 같다.

            ![01-3%20%E1%84%86%E1%85%A1%E1%84%8F%E1%85%A6%E1%86%BA%E1%84%80%E1%85%AA%20%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%200973347cb86f481aae39534185ca0f54/Untitled%205.png](01-3%20%E1%84%86%E1%85%A1%E1%84%8F%E1%85%A6%E1%86%BA%E1%84%80%E1%85%AA%20%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%200973347cb86f481aae39534185ca0f54/Untitled%205.png)

            코드 및 결과 값

        - fish_data의 데이터 49개 중에 도미가 35개로 다수를 차지하므로 어떤 데이터를 넣어도 무조건 도미로 예측할 것이다.

            ![01-3%20%E1%84%86%E1%85%A1%E1%84%8F%E1%85%A6%E1%86%BA%E1%84%80%E1%85%AA%20%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%200973347cb86f481aae39534185ca0f54/Untitled%206.png](01-3%20%E1%84%86%E1%85%A1%E1%84%8F%E1%85%A6%E1%86%BA%E1%84%80%E1%85%AA%20%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%200973347cb86f481aae39534185ca0f54/Untitled%206.png)

            kn49 모델은 도미만 맞추므로 위와 같은 값이 나옴

## 도미와 빙어 분류

도미와 빙어를 구분하기 위해 머신러닝 프로그램을 만들었다.

1. 도미 35마리와 빙어 14마리의 길이와 무게를 파이썬 리스트로 만든다.
2. 2차원 리스트로 데이터를 준비
3. k-NN 알고리즘을 사용(가장 가까운 5개의 데이터 참고)
4. fit(), score(), predict() 메서드를 사용.

## 최종 코드 by using Google Colab

[Google Colaboratory](https://colab.research.google.com/drive/1Pw7PEjwZutJ955VRarMG--qeceXAYQ5R?usp=sharing)

made by Sunwung Lee

---

## 핵심 패키지와 함수

- matplotlib
    - scatter()는 산점도를 그리는 matplotlib 함수.
    - 처음 두 개의 parameter로 x축과 y축 값을 전달.
    - 이 값은 파이썬 list 또는 numpy 배열.
    - c parameter로 색깔을 지정. ([https://bit.ly/matplotlib_prop_cycle](https://bit.ly/matplotlib_prop_cycle)))
    - marker parameter로 마커 스타일을 저장. ([https://bit.ly/matplotlib_marker](https://bit.ly/matplotlib_marker))
- scikit-learn
    - KNeighborsClassifier()는 k-최근접 이웃 분류 모델을 만드는 scikit-learn 클래스다.
    - p 매개변수로 거리를 재는 방법을 지정한다.
        - p = 1일 경우, [맨해튼 거리](https://ko.wikipedia.org/wiki/%EB%A7%A8%ED%95%B4%ED%8A%BC_%EA%B1%B0%EB%A6%AC)를 사용
        - p = 2일 경우, [유클리디안 거리](https://ko.wikipedia.org/wiki/%EC%9C%A0%ED%81%B4%EB%A6%AC%EB%93%9C_%EA%B1%B0%EB%A6%AC)를 사용

        기본 값은 2 이다.

    - fit(): scikit-learn 모델을 훈련할 때 사용하는 method. 
    처음 두 parameters로 훈련에 사용할 특성과 정답 데이터를 전달.
    - predict(): scikit-learn 모델을 훈련하고 예측할 때 사용하는 method.
    특성 데이터 하나만 매개변수로 받음.
    - score(): 훈련된 scikit-learn 모델의 성능을 측정.
    처음 두 parameters로 특성과 정답 데이터를 전달.
    predict()로 예측을 수행한 다음 분류 모델일 경우 정답과 비교하여 올바르게 예측한 개수의 비율을 반환.